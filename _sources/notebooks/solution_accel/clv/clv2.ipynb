{"cells":[{"cell_type":"markdown","source":["# 将来消費を推定する\n\n前回のノートでは、定額制ではないモデルの顧客が、時間の経過とともにどのように離脱していくかを検証しました。 小売企業と顧客の間に書面契約がない場合、顧客が継続的な関係から脱落する確率は、他の顧客と比較した過去のエンゲージメントパターンに基づいて推定するしかありません。顧客が積極的に関与し続ける確率を理解することは、それ自体が非常に価値のあることです。 しかし、さらに一歩進んで、予測される将来のエンゲージメントからどれだけの収益や利益が得られるかを計算することができます。\n\nそのためには、将来の購入イベントに関連する金銭的価値を計算するモデルを構築する必要があります。 このモデルの目的は、そのようなモデルを導き出し、生涯確率と組み合わせて推定顧客生涯価値を導き出すことです。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc88618d-bd08-46e3-8710-7ef16e417426"}}},{"cell_type":"markdown","source":["### Step 1: 環境の設定\n\nこれまでと同様に、クラスタに以下のライブラリを[インストール](https://docs.databricks.com/libraries.html#workspace-libraries)・[アタッチ](https://docs.databricks.com/libraries.html#install-a-library-on-a-cluster)する必要があります。また、クラスタのランライムには **Databricks ML runtime** ver 6.5以上を指定する必要があります。:</p>\n\n* xlrd\n* lifetimes==0.10.1\n* nbconvert\n\nさらに、前のノートブックと同様に、UCI Machine Learning Repositoryから入手できる[Online Retail Data Set](http://archive.ics.uci.edu/ml/datasets/Online+Retail)を、 `/FileStore/tables/online_retail/`　フォルダにロードする必要があります。\n\n(訳者注: このNotebookではすでにデータ形式が利用しやすいCSV形式になっているので、xlrd, nbconvertはインストール不要になっています。)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b72dc5db-d319-47d7-81df-740bb4e48389"}}},{"cell_type":"code","source":["%pip install lifetimes==0.10.1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"lifetimesライブラリのインストール","showTitle":true,"inputWidgets":{},"nuid":"875682ab-da3c-4ef8-8b3f-c19f3400c2a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh\nwget 'https://sajpstorage.blob.core.windows.net/demo-asset-workshop2021/Online_Retail-93ac4.csv'\ncp Online_Retail-93ac4.csv /dbfs/FileStore/tables/online_retail/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"879b962e-8510-4c1d-8156-8bd9ad9c20f8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\n\n# 対象のxlsxファイルのパスを取得\nxlsx_filename = dbutils.fs.ls('file:///dbfs/FileStore/tables/online_retail')[0][0]\n\n# 上記ファイルのデータのスキーマを設定(既知とする)\norders_schema = {\n  'InvoiceNo':np.str,\n  'StockCode':np.str,\n  'Description':np.str,\n  'Quantity':np.int64,\n#  'InvoiceDate':np.datetime64,\n  'InvoiceDate':np.str,\n  'UnitPrice':np.float64,\n  'CustomerID':np.str,\n  'Country':np.str  \n  }\n\n#　元のファイルがCSVになっているので、そのまま読み込む\norders_pd = pd.read_csv(\n  xlsx_filename, \n  sep=',',\n  #sheet_name='Online Retail',\n  header=0, # 第一行目はヘッダーになっている\n  dtype=orders_schema,\n  parse_dates=['InvoiceDate']\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad0e59d7-b371-48ae-b256-57e8a8142006"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["顧客の購入に関連する金銭的価値を調べるためには、オンライン小売注文データセットの各販売額を計算する必要があります。そのため、QuantityにUnitPriceを乗じて、新しいSalesAmountフィールドを作成します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3025b55-abee-421f-ae0e-b456bd34f6a5"}}},{"cell_type":"code","source":["# 売上を算出: SalesAmount =  quantity * unit price\norders_pd['SalesAmount'] = orders_pd['Quantity'] * orders_pd['UnitPrice']\n\norders_pd.head(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d401e83-5970-447b-9807-e31039ce7b33"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["上記のPandasデータをSpark上で使えるように準備する"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f18836b7-1312-4449-b424-9bb50fab9f4c"}}},{"cell_type":"code","source":["# データ変換: pandas DF から Spark DF　へ\norders = spark.createDataFrame(orders_pd)\n\n# SparkDFをクエリで使うために\"orders\"という名前のTemp Viewを作成\norders.createOrReplaceTempView('orders') "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39b686db-1715-46dc-9264-9166f85ce9a4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 2: データの探索\n\nデータセットの購入頻度のパターンを調べるには、前のノートブックのステップ2のセクションを参照してください。 ここでは、顧客の消費に関するパターンを調べたいと思います。 \n\nはじめに、顧客の典型的な1日あたりの購入額を見てみましょう。 顧客生涯の計算と同様に、同じ日に複数回の購入があっても同じ購入イベントとみなすため、1日単位でグループ化します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba0f2cde-dbdc-4fe3-8ee3-f9726e2f0cd5"}}},{"cell_type":"code","source":["%sql -- 顧客毎の日毎の購入額\n\nSELECT\n  CustomerID,\n  TO_DATE(InvoiceDate) as InvoiceDate,\n  SUM(SalesAmount) as SalesAmount\nFROM orders\nGROUP BY CustomerID, TO_DATE(InvoiceDate)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13645834-c7b3-424e-82f2-f8f5668d1d28"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["1日の消費額の範囲は非常に広く、1日に£70,000以上を購入する顧客もいます。背景にあるビジネスの知識がないため、この段階で支出が期待値と一致しているのか、それとも排除すべき異常値なのかを判断することはできません。 \n\nまた、マイナスの値が非常に多いことにも注目してください。これはリターンに関連している可能性が高いです。 この点については後ほど詳しく説明しますが、今回はサイトで観察されるアクティビティの分布を把握するために、調べる範囲を狭めます。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"976714dd-d863-49cd-86dd-f8054ada4adb"}}},{"cell_type":"code","source":["%sql -- 顧客毎の日毎の購入額 (+条件: 日毎の売上高: 0 - 2500ポンド)\n\nSELECT\n  CustomerID,\n  TO_DATE(InvoiceDate) as InvoiceDate,\n  SUM(SalesAmount) as SalesAmount\nFROM orders\nGROUP BY CustomerID, TO_DATE(InvoiceDate)\nHAVING SalesAmount BETWEEN 0 AND 2500"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c81ab22d-b3a0-4ecd-981d-a2347b9f7de0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["この限定した範囲での1日の支出額の分布は、200 -- 400ポンドを中心とし、それ以上の支出額に向かってロングテールになっています。これが正規分布（ガウス分布）でないことは明らかです。\n\nこのような分布パターンは、個々の顧客の支出パターンにも見られます。購入回数の多い顧客に焦点を当てると、支出パターンは様々ですが、この右に偏ったパターンが続いていることがわかります。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4737602e-753f-49f4-af46-64a8ef47e13f"}}},{"cell_type":"code","source":["%sql -- 買い物頻度による顧客ランキング (Top5)\n\nSELECT\n  CustomerID,\n  COUNT(DISTINCT TO_DATE(InvoiceDate)) as Frequency\nFROM orders\nGROUP BY CustomerID\nORDER BY Frequency DESC\nLIMIT 5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9d5d5ef-1a10-40d9-b77a-a1a901cc98ff"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql -- 上記の結果から、上位3顧客の日毎の購入額の分布を調べる\n\nSELECT\n  CustomerID,\n  TO_DATE(InvoiceDate) as InvoiceDate,\n  SUM(SalesAmount) as SalesAmount\nFROM orders\nWHERE CustomerID IN (14911, 12748, 17841)\nGROUP BY CustomerID, TO_DATE(InvoiceDate)\nORDER BY CustomerID"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5521fe43-f9f1-4609-9f70-98e841893482"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["このデータセットには、もう少し検討すべき点があります。その中でも、まずは顧客ごとの指標を算出する必要があります。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cea1b7d3-3027-40af-96c1-8cba75873156"}}},{"cell_type":"markdown","source":["### Step 3: 顧客メトリックを算出する\n\n今回のデータセットには、生の取引履歴が含まれています。 前回と同様に、頻度(Frequency)、Age(T)、直近性(Recency)という顧客ごとの指標の計算が必要です。また同時に、金銭的価値の指標の算出も必要になります。:</p>\n\n* **Frequency** - 観測期間中の取引(買い物)回数。ただし、初回購入は除く。つまり、(全取引回数 - 1)。日毎にカウント。つまり、同日に複数回取引があっても1回とカウントする。\n* **Age (T)** - 経過日数, 初めての取引発生した日から現在の日付（またはデータセットの最終の日)\n* **Recency** - 直近の取引があった時点のAge。つまり、初回の取引の日から直近(最後の)取引があった日までの経過日数。\n* **Monetary Value** - 金銭的価値。顧客がリピート購入する際の1トランザクションあたりの平均消費額(マージンやその他の金銭的価値がある場合は、それを代用することも可能です。)\n\n顧客年齢などの指標を計算する際には、データセットがいつ終了するかを考慮する必要があることに注意してください。 これらの指標を今日の日付を基準にして計算すると、誤った結果になる可能性があります。 そこで、データセットの最後の日付を特定し、それをすべての計算において「*今日の日付*」と定義します。\n\nこれらのメトリクスを導き出すために、[lifetimesライブラリ](https://lifetimes.readthedocs.io/en/latest/lifetimes.html)の組み込み機能を利用することができます。前のノートブックのコードと同様、呼び出されているメソッドはほぼ同じものを使用しています。唯一の違いは、金額の尺度としてSalesAmountフィールドを使用するようにメソッドで指定していることです。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b44b1aaa-aa14-4b65-ba88-d1b556b7fade"}}},{"cell_type":"code","source":["import lifetimes\n\n# 最後のトランザクション発生日をデータセットのエンドポイント(=「今日」)と見なす。\ncurrent_date = orders_pd['InvoiceDate'].max()\n\n# 必要な顧客メトリックをlifetimesライブラリを使って算出する\nmetrics_pd = (\n  lifetimes.utils.summary_data_from_transaction_data(\n    orders_pd,\n    customer_id_col='CustomerID',\n    datetime_col='InvoiceDate',\n    observation_period_end = current_date, \n    freq='D',\n    monetary_value_col='SalesAmount'  # use sales amount to determine monetary value\n    )\n  )\n\n# 最初の数行を確認する\nmetrics_pd.head(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"lifetimesライブラリによる算出","showTitle":true,"inputWidgets":{},"nuid":"c9395132-003b-4297-921f-16cb743c3510"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["前回と同様に、大規模なデータセットを扱う際を想定して、これらの値を並列処理で計算できるようにSparkを使って同じデータセットを生成する方法を見ていきます。これには以下の2通りの方法があります。 \n\n1. SQLステートメントを使用する方法\n1. Python(プログラマティックSQL API)を使用する方法\n\nこれら2つを順に見ていきます。\n\nコードは可能な限り前のノートブックと一貫性を保つようにしましたが、金額ロジックに必要な追加ロジックは一部そうでない部分があります。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2954df5d-1f6c-41f9-8cc1-d2abbde0a271"}}},{"cell_type":"code","source":["# SQL文を記述\nsql = '''\n  SELECT\n    a.customerid as CustomerID,\n    CAST(COUNT(DISTINCT a.transaction_at) - 1 as float) as frequency,\n    CAST(DATEDIFF(MAX(a.transaction_at), a.first_at) as float) as recency,\n    CAST(DATEDIFF(a.current_dt, a.first_at) as float) as T,\n    CASE                                              -- MONETARY VALUE CALCULATION\n      WHEN COUNT(DISTINCT a.transaction_at)=1 THEN 0    -- 0 if only one order\n      ELSE\n        SUM(\n          CASE WHEN a.first_at=a.transaction_at THEN 0  -- daily average of all but first order\n          ELSE a.salesamount\n          END\n          ) / (COUNT(DISTINCT a.transaction_at)-1)\n      END as monetary_value    \n  FROM ( -- customer order history\n    SELECT\n      x.customerid,\n      z.first_at,\n      x.transaction_at,\n      y.current_dt,\n      x.salesamount                  \n    FROM (                                            -- customer daily summary\n      SELECT \n        customerid, \n        TO_DATE(invoicedate) as transaction_at, \n        SUM(SalesAmount) as salesamount               -- SALES AMOUNT ADDED\n      FROM orders \n      GROUP BY customerid, TO_DATE(invoicedate)\n      ) x\n    CROSS JOIN (SELECT MAX(TO_DATE(invoicedate)) as current_dt FROM orders) y                                -- current date (according to dataset)\n    INNER JOIN (SELECT customerid, MIN(TO_DATE(invoicedate)) as first_at FROM orders GROUP BY customerid) z  -- first order per customer\n      ON x.customerid=z.customerid\n    WHERE x.customerid IS NOT NULL\n    ) a\n  GROUP BY a.customerid, a.current_dt, a.first_at\n  ORDER BY CustomerID\n  '''\n\n# SQLを実行して、結果をSpark Dataframeで受ける\nmetrics_sql = spark.sql(sql)\n\n# 結果を確認する\ndisplay(metrics_sql)  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQLを用いた算出","showTitle":true,"inputWidgets":{},"nuid":"4d66b53f-e640-4d02-a006-1e3d86017e67"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, datediff, max, min, countDistinct, count, sum, when\nfrom pyspark.sql.types import *\n\n# 有効な顧客注文を含むレコードのみを抜き出す\nx = (\n    orders\n      .where(orders.CustomerID.isNotNull())\n      .withColumn('transaction_at', to_date(orders.InvoiceDate))\n      .groupBy(orders.CustomerID, 'transaction_at')\n      .agg(sum(orders.SalesAmount).alias('salesamount'))   # SALES AMOUNT\n    )\n\n# 最後のトランザクション発生日を取得する\ny = (\n  orders\n    .groupBy()\n    .agg(max(to_date(orders.InvoiceDate)).alias('current_dt'))\n  )\n\n\n# 顧客毎の最初のトランザクション日時を算出\nz = (\n  orders\n    .groupBy(orders.CustomerID)\n    .agg(min(to_date(orders.InvoiceDate)).alias('first_at'))\n  )\n\n\n# 顧客の購入履歴を日時情報と結合させる\na = (x\n    .crossJoin(y)\n    .join(z, x.CustomerID==z.CustomerID, how='inner')\n    .select(\n      x.CustomerID.alias('customerid'), \n      z.first_at, \n      x.transaction_at,\n      x.salesamount,               # SALES AMOUNT\n      y.current_dt\n      )\n    )\n\n# 顧客毎に関連するメトリックを算出する\nmetrics_api = (a\n           .groupBy(a.customerid, a.current_dt, a.first_at)\n           .agg(\n             (countDistinct(a.transaction_at)-1).cast(FloatType()).alias('frequency'),\n             datediff(max(a.transaction_at), a.first_at).cast(FloatType()).alias('recency'),\n             datediff(a.current_dt, a.first_at).cast(FloatType()).alias('T'),\n             when(countDistinct(a.transaction_at)==1,0)                           # MONETARY VALUE\n               .otherwise(\n                 sum(\n                   when(a.first_at==a.transaction_at,0)\n                     .otherwise(a.salesamount)\n                   )/(countDistinct(a.transaction_at)-1)\n                 ).alias('monetary_value')\n               )\n           .select('customerid','frequency','recency','T','monetary_value')\n           .orderBy('customerid')\n          )\n\n# 結果の確認\ndisplay(metrics_api)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Python(Spark SQL API)による算出","showTitle":true,"inputWidgets":{},"nuid":"7e7401b7-9a34-4d8a-a3f1-04a371de2914"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["以前のように、いくつかの要約統計を使って、SQLで生成されたデータセットとlifetimesライブラリで生成されたデータセットが同じであることを確認していきましょう:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebff0206-55e8-4c51-8364-6ec32758be84"}}},{"cell_type":"code","source":["# summary data from lifetimes\nmetrics_pd.describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"lifetimesライブラリから算出した結果のサマリ統計","showTitle":true,"inputWidgets":{},"nuid":"3efae54b-ef7f-4183-9e41-1bac8fc8a8e9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# summary data from SQL statement\nmetrics_sql.toPandas().describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQLから算出した結果のサマリ統計","showTitle":true,"inputWidgets":{},"nuid":"6fb686db-6b70-496f-9c68-fcb06b25bb8b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# summary data from pyspark.sql API\nmetrics_api.toPandas().describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Python(Spark SQL API)で算出した結果のサマリ統計","showTitle":true,"inputWidgets":{},"nuid":"a44d44cf-d3ab-485e-90db-dbacf2b4a0f6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Eこの計算を発展させて、キャリブレーション期間とホールドアウト期間の値を導き出すと、以下のようなロジックになります。:\n\n注：ここでもウィジェットを使ってホールドアウト期間の日数を定義しています。."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37bfc098-8dae-40e9-a3f1-e8f14ac4ee95"}}},{"cell_type":"code","source":["# ホールドアウト期間を指定するためのウィジットを定義(デフォルト: 90日)\ndbutils.widgets.text('holdout days', '90')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f2c6d28-a0f2-46aa-824f-e3e5361b1fb8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from datetime import timedelta\n\n# 最後のトランザクション発生日をデータセットのエンドポイント(=「今日」)と見なす。\ncurrent_date = orders_pd['InvoiceDate'].max()\n\n# キャリブレーション期間の最終日を算出\nholdout_days = int(dbutils.widgets.get('holdout days'))\ncalibration_end_date = current_date - timedelta(days = holdout_days)\n\n# 必要な顧客メトリックを算出する\nmetrics_cal_pd = (\n  lifetimes.utils.calibration_and_holdout_data(\n    orders_pd,\n    customer_id_col='CustomerID',\n    datetime_col='InvoiceDate',\n    observation_period_end = current_date,\n    calibration_period_end=calibration_end_date,\n    freq='D',\n    monetary_value_col='SalesAmount'  # use sales amount to determine monetary value\n    )\n  )\n\n# 結果を数行表示して確認\nmetrics_cal_pd.head(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"lifetimesライブラリによる算出","showTitle":true,"inputWidgets":{},"nuid":"4d727283-2432-4662-8f98-59aa0d26f3de"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["SQLおよびPython(Spark SQL API)での実装は以下のとおりです。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50c99585-672f-41fe-a62c-56cc2b856873"}}},{"cell_type":"code","source":["sql = '''\nWITH CustomerHistory \n  AS (\n    SELECT  -- nesting req'ed b/c can't SELECT DISTINCT on widget parameter\n      m.*,\n      getArgument('holdout days') as duration_holdout\n    FROM (\n      SELECT\n        x.customerid,\n        z.first_at,\n        x.transaction_at,\n        y.current_dt,\n        x.salesamount\n      FROM (                                            -- CUSTOMER DAILY SUMMARY\n        SELECT \n          customerid, \n          TO_DATE(invoicedate) as transaction_at, \n          SUM(SalesAmount) as salesamount \n        FROM orders \n        GROUP BY customerid, TO_DATE(invoicedate)\n        ) x\n      CROSS JOIN (SELECT MAX(TO_DATE(invoicedate)) as current_dt FROM orders) y                                -- current date (according to dataset)\n      INNER JOIN (SELECT customerid, MIN(TO_DATE(invoicedate)) as first_at FROM orders GROUP BY customerid) z  -- first order per customer\n        ON x.customerid=z.customerid\n      WHERE x.customerid is not null\n      ) m\n  )\nSELECT\n    a.customerid as CustomerID,\n    a.frequency as frequency_cal,\n    a.recency as recency_cal,\n    a.T as T_cal,\n    COALESCE(a.monetary_value,0.0) as monetary_value_cal,\n    COALESCE(b.frequency_holdout, 0.0) as frequency_holdout,\n    COALESCE(b.monetary_value_holdout, 0.0) as monetary_value_holdout,\n    a.duration_holdout\nFROM ( -- CALIBRATION PERIOD CALCULATIONS\n    SELECT\n        p.customerid,\n        CAST(p.duration_holdout as float) as duration_holdout,\n        CAST(DATEDIFF(MAX(p.transaction_at), p.first_at) as float) as recency,\n        CAST(COUNT(DISTINCT p.transaction_at) - 1 as float) as frequency,\n        CAST(DATEDIFF(DATE_SUB(p.current_dt, int(p.duration_holdout) ), p.first_at) as float) as T,\n        CASE                                              -- MONETARY VALUE CALCULATION\n          WHEN COUNT(DISTINCT p.transaction_at)=1 THEN 0    -- 0 if only one order\n          ELSE\n            SUM(\n              CASE WHEN p.first_at=p.transaction_at THEN 0  -- daily average of all but first order\n              ELSE p.salesamount\n              END\n              ) / (COUNT(DISTINCT p.transaction_at)-1)\n          END as monetary_value    \n    FROM CustomerHistory p\n    WHERE p.transaction_at < DATE_SUB( p.current_dt, int(p.duration_holdout) )   -- LIMIT THIS QUERY TO DATA IN THE CALIBRATION PERIOD\n    GROUP BY p.customerid, p.duration_holdout, p.current_dt, p.first_at\n  ) a\nLEFT OUTER JOIN ( -- HOLDOUT PERIOD CALCULATIONS\n  SELECT\n    p.customerid,\n    CAST(COUNT(DISTINCT p.transaction_at) as float) as frequency_holdout,\n    AVG(p.salesamount) as monetary_value_holdout      -- MONETARY VALUE CALCULATION\n  FROM CustomerHistory p\n  WHERE \n    p.transaction_at >= DATE_SUB(p.current_dt, int(p.duration_holdout) ) AND  -- LIMIT THIS QUERY TO DATA IN THE HOLDOUT PERIOD\n    p.transaction_at <= p.current_dt\n  GROUP BY p.customerid\n  ) b\n  ON a.customerid=b.customerid\nORDER BY CustomerID\n'''\n\nmetrics_cal_sql = spark.sql(sql)\ndisplay(metrics_cal_sql)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQLにより算出","showTitle":true,"inputWidgets":{},"nuid":"ba2065e4-a148-499e-87ca-65b4a84ac9cf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import avg, date_sub, coalesce, lit, expr\n\n# valid customer orders\nx = (\n  orders\n    .where(orders.CustomerID.isNotNull())\n    .withColumn('transaction_at', to_date(orders.InvoiceDate))\n    .groupBy(orders.CustomerID, 'transaction_at')\n    .agg(sum(orders.SalesAmount).alias('salesamount'))\n  )\n\n# calculate last date in dataset\ny = (\n  orders\n    .groupBy()\n    .agg(max(to_date(orders.InvoiceDate)).alias('current_dt'))\n  )\n\n# calculate first transaction date by customer\nz = (\n  orders\n    .groupBy(orders.CustomerID)\n    .agg(min(to_date(orders.InvoiceDate)).alias('first_at'))\n  )\n\n# combine customer history with date info (CUSTOMER HISTORY)\np = (x\n    .crossJoin(y)\n    .join(z, x.CustomerID==z.CustomerID, how='inner')\n    .withColumn('duration_holdout', lit(int(dbutils.widgets.get('holdout days'))))\n    .select(\n      x.CustomerID.alias('customerid'),\n      z.first_at, \n      x.transaction_at, \n      y.current_dt, \n      x.salesamount,\n      'duration_holdout'\n      )\n     .distinct()\n    )\n\n# calculate relevant metrics by customer\n# note: date_sub requires a single integer value unless employed within an expr() call\na = (p\n       .where(p.transaction_at < expr('date_sub(current_dt, duration_holdout)')) \n       .groupBy(p.customerid, p.current_dt, p.duration_holdout, p.first_at)\n       .agg(\n         (countDistinct(p.transaction_at)-1).cast(FloatType()).alias('frequency_cal'),\n         datediff( max(p.transaction_at), p.first_at).cast(FloatType()).alias('recency_cal'),\n         datediff( expr('date_sub(current_dt, duration_holdout)'), p.first_at).cast(FloatType()).alias('T_cal'),\n         when(countDistinct(p.transaction_at)==1,0)\n           .otherwise(\n             sum(\n               when(p.first_at==p.transaction_at,0)\n                 .otherwise(p.salesamount)\n               )/(countDistinct(p.transaction_at)-1)\n             ).alias('monetary_value_cal')\n       )\n    )\n\nb = (p\n      .where((p.transaction_at >= expr('date_sub(current_dt, duration_holdout)')) & (p.transaction_at <= p.current_dt) )\n      .groupBy(p.customerid)\n      .agg(\n        countDistinct(p.transaction_at).cast(FloatType()).alias('frequency_holdout'),\n        avg(p.salesamount).alias('monetary_value_holdout')\n        )\n   )\n\nmetrics_cal_api = (\n                 a\n                 .join(b, a.customerid==b.customerid, how='left')\n                 .select(\n                   a.customerid.alias('CustomerID'),\n                   a.frequency_cal,\n                   a.recency_cal,\n                   a.T_cal,\n                   a.monetary_value_cal,\n                   coalesce(b.frequency_holdout, lit(0.0)).alias('frequency_holdout'),\n                   coalesce(b.monetary_value_holdout, lit(0.0)).alias('monetary_value_holdout'),\n                   a.duration_holdout\n                   )\n                 .orderBy('CustomerID')\n              )\n\ndisplay(metrics_cal_api)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Python(Spark SQL API)を使って算出","showTitle":true,"inputWidgets":{},"nuid":"bc849c42-c5bb-4cfe-978d-b34dafd7f182"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["それぞれの結果を比較する"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33756652-a47c-4a9a-8f28-37047b740e8e"}}},{"cell_type":"code","source":["# summary data from lifetimes\nmetrics_cal_pd.describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"lifetimesライブラリから算出した結果のサマリ統計","showTitle":true,"inputWidgets":{},"nuid":"b4298fcd-4ee1-439e-b6db-dcd75c782108"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# summary data from SQL statement\nmetrics_cal_sql.toPandas().describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQLから算出した結果のサマリ統計","showTitle":true,"inputWidgets":{},"nuid":"995ee314-cd37-4086-9588-384c1a53ce7b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# summary data from pyspark.sql API\nmetrics_cal_api.toPandas().describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Python(Spark SQL API)から算出した結果のサマリ統計","showTitle":true,"inputWidgets":{},"nuid":"64759fc9-2518-4ca5-8a9a-ed1a70f7557e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["lifetimesライブラリで計算された金銭的なホールドアウト値を注意深く見てみましょう。算出された値は、Sparkコードで算出された値よりもかなり低いことに気づくはずです。これは、lifetimesライブラリが、取引日の合計を平均する代わりに、特定の取引日の個々のラインアイテムを平均しているためです。 lifetimesライブラリの管理者にコードの変更のリクエストをしているのですが、まだ未対応のままになっています。ここでは取引日合計の平均が正しいと思われるので、このノートブックの残りの部分ではそれを使用します。\n\nlifetimesライブラリで生成される値と同じ値をSparkで生成したい場合は、以下の2つのセルを参照ください。\nここでSQLを使用してlifetimesライブラリのロジックを再現してあります。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8709f1d-ce83-4190-b234-5fca86af366c"}}},{"cell_type":"code","source":["sql = '''\nWITH CustomerHistory \n  AS (\n    SELECT  -- nesting req'ed b/c can't SELECT DISTINCT on widget parameter\n      m.*,\n      getArgument('holdout days') as duration_holdout\n    FROM (\n      SELECT\n        x.customerid,\n        z.first_at,\n        x.transaction_at,\n        y.current_dt,\n        x.salesamount\n      FROM (                                            -- CUSTOMER DAILY SUMMARY\n        SELECT \n          customerid, \n          TO_DATE(invoicedate) as transaction_at, \n          SUM(SalesAmount) as salesamount \n        FROM orders \n        GROUP BY customerid, TO_DATE(invoicedate)\n        ) x\n      CROSS JOIN (SELECT MAX(TO_DATE(invoicedate)) as current_dt FROM orders) y                                -- current date (according to dataset)\n      INNER JOIN (SELECT customerid, MIN(TO_DATE(invoicedate)) as first_at FROM orders GROUP BY customerid) z  -- first order per customer\n        ON x.customerid=z.customerid\n      WHERE x.customerid is not null\n      ) m\n  )\nSELECT\n    a.customerid as CustomerID,\n    a.frequency as frequency_cal,\n    a.recency as recency_cal,\n    a.T as T_cal,\n    COALESCE(a.monetary_value,0.0) as monetary_value_cal,\n    COALESCE(b.frequency_holdout, 0.0) as frequency_holdout,\n    COALESCE(b.monetary_value_holdout, 0.0) as monetary_value_holdout,\n    a.duration_holdout\nFROM ( -- CALIBRATION PERIOD CALCULATIONS\n    SELECT\n        p.customerid,\n        CAST(p.duration_holdout as float) as duration_holdout,\n        CAST(DATEDIFF(MAX(p.transaction_at), p.first_at) as float) as recency,\n        CAST(COUNT(DISTINCT p.transaction_at) - 1 as float) as frequency,\n        CAST(DATEDIFF(DATE_SUB(p.current_dt, int(p.duration_holdout) ), p.first_at) as float) as T,\n        CASE                                              -- MONETARY VALUE CALCULATION\n          WHEN COUNT(DISTINCT p.transaction_at)=1 THEN 0    -- 0 if only one order\n          ELSE\n            SUM(\n              CASE WHEN p.first_at=p.transaction_at THEN 0  -- daily average of all but first order\n              ELSE p.salesamount\n              END\n              ) / (COUNT(DISTINCT p.transaction_at)-1)\n          END as monetary_value    \n    FROM CustomerHistory p\n    WHERE p.transaction_at < DATE_SUB(p.current_dt, int( p.duration_holdout) )  -- LIMIT THIS QUERY TO DATA IN THE CALIBRATION PERIOD\n    GROUP BY p.customerid, p.duration_holdout, p.current_dt, p.first_at\n  ) a\nLEFT OUTER JOIN ( -- HOLDOUT PERIOD CALCULATIONS\n  SELECT\n    p.customerid,\n    CAST(COUNT(DISTINCT TO_DATE(p.invoicedate)) as float) as frequency_holdout,\n    AVG(p.salesamount) as monetary_value_holdout      -- MONETARY VALUE CALCULATION\n  FROM orders p\n  CROSS JOIN (SELECT MAX(TO_DATE(invoicedate)) as current_dt FROM orders) q                                -- current date (according to dataset)\n  INNER JOIN (SELECT customerid, MIN(TO_DATE(invoicedate)) as first_at FROM orders GROUP BY customerid) r  -- first order per customer\n    ON p.customerid=r.customerid\n  WHERE \n    p.customerid is not null AND\n    TO_DATE(p.invoicedate) >= DATE_SUB(q.current_dt, int( getArgument('holdout days') ) ) AND  -- LIMIT THIS QUERY TO DATA IN THE HOLDOUT PERIOD\n    TO_DATE(p.invoicedate) <= q.current_dt\n  GROUP BY p.customerid\n  ) b\n  ON a.customerid=b.customerid\nORDER BY CustomerID\n'''\n\nmetrics_cal_sql_alt = spark.sql(sql)\ndisplay(metrics_cal_sql_alt)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQLを用いて、lifetimeライブラリのmonetary_holdout値を算出","showTitle":true,"inputWidgets":{},"nuid":"bf27edac-eeee-4b36-b4f7-beac5c40cd35"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# lifetimesライブラリによる算出結果の統計サマリ (比較用)\nmetrics_cal_pd.describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8654fce6-b91e-4652-8621-08a732c2189e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# SQLによる算出結果の統計サマリ\n# \"monetary_value_holdout\"はlifetmesライブラリの実装と同じ仕様にしている\nmetrics_cal_sql_alt.toPandas().describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebe88fc8-f998-4abb-b207-d9f07ada10f6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["これ以降は、前回のノートブックと同様に、リピート購入がある顧客に限定して分析を行います。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fce9221f-3be3-45a1-92b2-d5474d554564"}}},{"cell_type":"code","source":["# リピート購入のない顧客を除外する (全データセット対象)\nfiltered = metrics_api.where(metrics_api.frequency > 0)\n\n# リピート購入のない顧客を除外する (キャリブレーション期間を対象)\nfiltered_cal = metrics_cal_api.where(metrics_cal_api.frequency_cal > 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e36798f2-ecd5-4fa6-8304-6d1078e4e0a9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["最後に、今回のデータセットに含まれる1日の合計値がマイナスになっているレコードに関しての考慮が必要になります。 \nこのデータセットの元となった小売業者についての文脈情報がなければ、これらのマイナス値は返品されたものだと考えられます。\n\n理想的には、返品された商品を元の購入商品と照合し、元の取引日に合わせて金額を調整することです。しかし、これを一貫して行うために必要な情報を持っていないため、マイナスのリターン値を毎日の取引合計に単純に含めることにします。これにより、1日の合計が£0以下になる場合は、その値を分析から除外します。実証実験の場以外では、これは一般的に適切ではありません。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d08f8b57-f880-46d1-8451-182bdeeb8357"}}},{"cell_type":"code","source":["# exclude dates with negative totals (see note above) \nfiltered = filtered.where(filtered.monetary_value > 0)\nfiltered_cal = filtered_cal.where(filtered_cal.monetary_value_cal > 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34a18822-72cb-4490-b7b2-9ebf021bb4d3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 4: 頻度と金銭的価値の独立性を検証する\n\nモデル化を進める前に、ここで採用するガンマ・ガンマモデル(前述の2つのガンマ分布にちなんで命名されている)は、顧客の購入頻度がその購入金額に影響しないことを前提としています。 これを検証することは重要で、頻度と金額の測定基準に対する単純なピアソン係数を計算することで可能になります。 今回の分析では、キャリブレーションとホールドアウトのサブセットを無視して、データセット全体に対してこれを行います。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cf19ed8-005b-42eb-bce4-cf79b2ea9712"}}},{"cell_type":"code","source":["# 相関係数(ピアソン係数)を算出\nfiltered.corr('frequency', 'monetary_value')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20b1fd5f-24bf-4312-a0ac-feaa61d0a541"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["完全に独立しているわけではありませんが、この2つの値の相関はかなり低いので、モデルのトレーニングを進めても問題ないと言えるでしょう。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ab930b8-29af-42c9-a578-8b3ba82dcac3"}}},{"cell_type":"markdown","source":["### Step 5: 消費モデルを訓練する\n\n測定基準が確立されたので、将来のトランザクションイベントから得られる金銭的価値を推定するモデルを訓練することができます。ここで使用するモデルは、[Gamma-Gamma model](http://www.brucehardie.com/notes/025/gamma_gamma.pdf)と呼ばれ、顧客集団の支出分布から得られるガンマ分布のパラメータに対して、個々の顧客の支出のガンマ分布を適合させるものです。計算は複雑ですが、lifetimesライブラリを用いて容易に算出できます。\n\nそのためにはまず、モデルが使用するL2正則化パラメータの最適な値を決定する必要があります。 そこで、前のノートブックと同様に[hyperopt](http://hyperopt.github.io/hyperopt/)を使って、効果的にパラメータ探索を行います。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41c5feed-d57a-4a82-9679-c69246fb8704"}}},{"cell_type":"code","source":["from hyperopt import hp, fmin, tpe, rand, SparkTrials, STATUS_OK, space_eval\n\nfrom lifetimes.fitters.gamma_gamma_fitter import GammaGammaFitter\n\n# サーチスペース(探索範囲)を定義\nsearch_space = hp.uniform('l2', 0.0, 1.0)\n\n# 評価関数を定義\ndef score_model(actuals, predicted, metric='mse'):\n  # メトリック名は小文字に揃える\n  metric = metric.lower()\n  \n  # 平均二乗誤差(MSE)と平均平方二乗誤差(RMSE)の場合\n  if metric=='mse' or metric=='rmse':\n    val = np.sum(np.square(actuals-predicted))/actuals.shape[0]\n    if metric=='rmse':\n        val = np.sqrt(val)\n  \n  # 平均絶対誤差(MAE)の場合\n  elif metric=='mae':\n    np.sum(np.abs(actuals-predicted))/actuals.shape[0]\n  \n  # その他の場合\n  else:\n    val = None\n  \n  return val\n\n\n# モデルトレーニングおよび評価の関数を定義する\ndef evaluate_model(param):\n  \n  # \"input_pd\"データフレームのレプリカを用意\n  data = inputs.value\n  \n  # 入力パラメータの抽出\n  l2_reg = param\n  \n  # Gramma-Gamma-Filterモデルのインスタンス化\n  model = GammaGammaFitter(penalizer_coef=l2_reg)\n  \n  # モデルのフィッティング(トレーニング)\n  model.fit(data['frequency_cal'], data['monetary_value_cal'])\n  \n  # モデルの評価\n  monetary_actual = data['monetary_value_holdout']\n  monetary_predicted = model.conditional_expected_average_profit(data['frequency_holdout'], data['monetary_value_holdout'])\n  mse = score_model(monetary_actual, monetary_predicted, 'mse')\n  \n  # スコアとステータスを戻り値として返す\n  return {'loss': mse, 'status': STATUS_OK}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9607085b-29da-4755-9084-e028c5ac61b8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Hyperoptの並列実行環境としてSparkのworkerを使用するように設定\nspark_trials = SparkTrials(parallelism=8)\n\n# Hpyeroptのパラメータ探索アルゴリズムの設定(今回はTPEを使用する)\nalgo = tpe.suggest\n\n# \"input_pd\"データフレームのコピーを各workerに配っておく\ninput_pd = filtered_cal.where(filtered_cal.monetary_value_cal > 0).toPandas()\ninputs = sc.broadcast(input_pd)\n\n# Hyper-parameter Tuningを実行 (かつ、MLflowでトラッキングする)\nargmin = fmin(\n  fn=evaluate_model,\n  space=search_space,\n  algo=algo,\n  max_evals=100,\n  trials=spark_trials\n  )\n\n# Broadcastしたデータをリリースする\ninputs.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de04cb27-e61c-4827-90cf-6a14ac75823c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 最適なハイパーパラメータを表示\nprint(space_eval(search_space, argmin))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3918732-ea59-406d-b279-62b021c98666"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["最適なL2値がわかったところで、最終的な消費モデルを作成してみましょう。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6c66a6b-f42e-4e11-9e40-7d709a353bd9"}}},{"cell_type":"code","source":["# ハイパーパラメータを取得\nl2_reg = space_eval(search_space, argmin)\n\n# 上記のハイパーパラメータを使って、モデルのインスタンス化\nspend_model = GammaGammaFitter(penalizer_coef=l2_reg)\n\n# モデルのトレーニング\nspend_model.fit(input_pd['frequency_cal'], input_pd['monetary_value_cal'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"624befba-31ff-40d1-93f7-6c4e99889c45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 6: 消費モデルの評価\n\nモデルの評価はとてもシンプルです。 予測値とホールドアウト期間の実績がどの程度一致しているかを調べ、そこからMSEを算出します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54fd8b45-9d8c-4b0c-9266-7eeebe7c63cc"}}},{"cell_type":"code","source":["# モデルの評価\nmonetary_actual = input_pd['monetary_value_holdout']\nmonetary_predicted = spend_model.conditional_expected_average_profit(input_pd['frequency_holdout'], input_pd['monetary_value_holdout'])\nmse = score_model(monetary_actual, monetary_predicted, 'mse')\n\nprint('MSE: {0}'.format(mse))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd44f7be-03a0-470d-9bf3-328e9f137e3e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["また、Gamma-Gammaモデルを説明した[原著論文](http://www.brucehardie.com/notes/025/gamma_gamma.pdf)で採用された手法である、予測された支出額と実際の支出額がどのように一致しているかを視覚的に確認することもできます。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdf48898-692c-47b7-9567-b2dc8fa15693"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\n# ヒストグラムのbins数を設定\nbins = 10\n\n# plot size\nplt.figure(figsize=(15, 5))\n\n# histogram plot values and presentation\nplt.hist(monetary_actual, bins, label='actual', histtype='bar', color='STEELBLUE', rwidth=0.99)\nplt.hist( monetary_predicted, bins, label='predict', histtype='step', color='ORANGE',  rwidth=0.99)\n\n# place legend on chart\nplt.legend(loc='upper right')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8be899f0-a048-4a58-87b5-2510ab05d640"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["10個のbins数でのヒストグラムでは、モデルは実際のデータとうまく一致しているように見えます。\nビンの数を増やすと、モデルはデータの残りの構造に従う一方で、最も価値の低い消費の発生を過小評価していることがわかります。\n興味深いことに、先に引用した論文でも同じようなパターンが観察されています。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c58ac70-b0b3-491e-99f3-3a471173ca29"}}},{"cell_type":"code","source":["# ヒストグラムのbins数を40に増やす\nbins = 40\n\n# plot size\nplt.figure(figsize=(15, 5))\n\n# histogram plot values and presentation\nplt.hist(monetary_actual, bins, label='actual', histtype='bar', color='STEELBLUE', rwidth=0.99)\nplt.hist( monetary_predicted, bins, label='predict', histtype='step', color='ORANGE',  rwidth=0.99)\n\n# place legend on chart\nplt.legend(loc='upper right')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fcecd35-675d-4b23-86fb-7aed921b31c5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 7: 顧客生涯価値を算出する\n\n消費モデルでは、将来の購入イベントから得られる可能性のある金銭的価値を計算することができます。 \n将来の支出イベントの可能性を計算するライフタイムモデルと組み合わせて使用することで、将来の期間における顧客生涯価値を導き出すことができます。\n\nこれを実証するには、まずライフタイムモデルをトレーニングする必要があります。 ここでは、BG/NBDモデルに、以前のノートブックの実行時に得られたL2パラメータ設定を使用します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24ae7861-b0e6-4216-831f-ea1f20c47952"}}},{"cell_type":"code","source":["from lifetimes.fitters.beta_geo_fitter import BetaGeoFitter\n\n# Spark-DFからPandas-DFに変換する\nlifetime_input_pd = filtered_cal.toPandas() \n\n# モデルのインスタンス作成(前回のNoteookのHyperparamチューニングの結果からパラメータを設定する)\nlifetimes_model = BetaGeoFitter(penalizer_coef=0.9995179967263891)\n\n# モデルのトレーニング\nlifetimes_model.fit(lifetime_input_pd['frequency_cal'], lifetime_input_pd['recency_cal'], lifetime_input_pd['T_cal'])\n\n# スコアリング\nfrequency_holdout_actual = lifetime_input_pd['frequency_holdout']\nfrequency_holdout_predicted = lifetimes_model.predict(lifetime_input_pd['duration_holdout'], lifetime_input_pd['frequency_cal'], lifetime_input_pd['recency_cal'], lifetime_input_pd['T_cal'])\nmse = score_model(frequency_holdout_actual, frequency_holdout_predicted, 'mse')\n\nprint('MSE: {0}'.format(mse))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca121a82-2965-4d05-bc73-0fc3374615b3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["では、これらを組み合わせてCLVを計算してみましょう。ここでは、毎月の割引率を1％として、12ヶ月間のCLVを計算します。\n\n注：CFOは通常、この種の計算に使用すべき割引率を定義します。 割引率が月単位の割引率であることを確認してください。 年単位の割引率が提供されている場合は、必ず[この式](https://www.experiglot.com/2006/06/07/how-to-convert-from-an-annual-rate-to-an-effective-periodic-rate-javascript-calculator/)を使って月単位に変換してください。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67ffcbc3-9818-43d3-80a3-a518c26ade05"}}},{"cell_type":"code","source":["clv_input_pd = filtered.toPandas()\n\n# 1年間のCLVを顧客毎に算出する\nclv_input_pd['clv'] = (\n  spend_model.customer_lifetime_value(\n    lifetimes_model, #the model to use to predict the number of future transactions\n    clv_input_pd['frequency'],\n    clv_input_pd['recency'],\n    clv_input_pd['T'],\n    clv_input_pd['monetary_value'],\n    time=12, # months\n    discount_rate=0.01 # monthly discount rate ~ 12.7% annually\n  )\n)\n\nclv_input_pd.head(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3d07845-b304-4d3f-8b0c-1c05bf226ceb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["CLVは、企業がターゲットを絞ったプロモーション活動を計画したり、カスタマー・エクイティを評価したりする際に使用される強力な指標です。そのため、私たちのモデルを使いやすい関数に変換して、バッチ、ストリーミング、インタラクティブなシナリオで使用できるようにすることができれば、非常に便利になります。\n\n前回のノートをご覧になった方は、私たちがどこに向かっているのかご存知でしょう。 ここでは、CLVの計算が1つのモデルではなく、2つのモデルに依存していることを指摘しておきます。 問題はありません。 ここでは、生涯モデルを消費モデルに関連するピクルス化されたアーティファクトとして保存し、消費モデル用に開発するカスタムラッパーで、生涯モデルを再インスタンス化して、予測に利用できるようにします。\n\nまずは、ライフタイムモデルを一時的に保存してみましょう。:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05308aa0-56f1-4946-917b-4c7a85ee6228"}}},{"cell_type":"code","source":["# lifetimesモデルを保存するテンポラリなパスを設定\nlifetimes_model_path = '/dbfs/tmp/lifetimes_model.pkl'\n\n# 以前の結果があれば削除する\ntry:\n  dbutils.fs.rm(lifetimes_model_path)\nexcept:\n  pass\n\n# 保存する\nlifetimes_model.save_model(lifetimes_model_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8fc9540-bcbc-488f-9f41-84f13ea1b434"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["それでは、消費モデルのカスタムラッパーを定義してみましょう。 ここで、`predict()` メソッドは非常にシンプルで、CLV値を返すだけです。 また、月の値と割引率が入力データに含まれていることを前提としています。\n\n`Predict()` メソッドのロジックを変更したほか、`load_context()` の定義を新たに設けました。 このメソッドは、[mlflow](https://mlflow.org/)モデルがインスタンス化されたときに呼び出されます。 ここでは、lifetimeモデルの成果物をロードします。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1718a25-8ff9-4b66-89d0-fb47f05bfa70"}}},{"cell_type":"code","source":["import mlflow \nimport mlflow.pyfunc\n\n# lifetimesモデルのラッパークラスを作成\nclass _clvModelWrapper(mlflow.pyfunc.PythonModel):\n  \n    def __init__(self, spend_model):\n      self.spend_model = spend_model\n        \n    def load_context(self, context):\n      # lifetimesライブラリからBase Model Fitterをimportしておく\n      from lifetimes.fitters.base_fitter import BaseFitter\n      \n      # モデルのインスタンスを作成\n      self.lifetimes_model = BaseFitter()\n      \n      # MLflowからlifetimesモデルをロードする\n      self.lifetimes_model.load_model(context.artifacts['lifetimes_model'])\n      \n    def predict(self, context, dataframe):\n      \n      # 入力データから各種パラメータを抽出\n      frequency = dataframe.iloc[:,0]\n      recency = dataframe.iloc[:,1]\n      T = dataframe.iloc[:,2]\n      monetary_value = dataframe.iloc[:,3]\n      months = int(dataframe.iloc[0,4])\n      discount_rate = float(dataframe.iloc[0,5])\n      \n      # CLV推定を実施する\n      results = pd.DataFrame(\n          self.spend_model.customer_lifetime_value(\n            self.lifetimes_model, #the model to use to predict the number of future transactions\n            frequency,\n            recency,\n            T,\n            monetary_value,\n            time=months,\n            discount_rate=discount_rate\n            ),\n          columns=['clv']\n          )\n      \n      return results[['clv']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cd4a586-ac0d-41ac-9b2b-3dfa61672c08"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["次に、消費モデルをmlflowに保存します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3872fea6-5d9e-49ba-96e9-18ba162f2e1e"}}},{"cell_type":"code","source":["# lifetimesライブラリをconda環境に追加\nconda_env = mlflow.pyfunc.get_default_conda_env()\nconda_env['dependencies'][-1]['pip'] += ['lifetimes==0.10.1'] # lifetimesのversionはノートブック前半でinstallしたversionに合わせる\n\n# モデルトレーニング実行をMLflowに保存する\nwith mlflow.start_run(run_name='deployment run') as run:\n  \n  # lifetimeモデルをartifact \"lifetime_model\"としてmlflowでトラックするための準備\n  artifacts = {'lifetimes_model': lifetimes_model_path}\n  \n  # MLflowでトラック\n  mlflow.pyfunc.log_model(\n    'model', \n    python_model=_clvModelWrapper(spend_model), \n    conda_env=conda_env,\n    artifacts=artifacts\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f74eef4-1bfe-478b-aeeb-594f3b4be280"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["先ほどと同様に、モデルから関数を作成します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ff0a2ca-1b00-4eea-8091-82735769a8b2"}}},{"cell_type":"code","source":["# 関数の戻り値のデータ型(スキーマ)を定義\nresult_schema = DoubleType()\n\n# MLflowに登録されたモデルをベースにした関数を定義する\nclv_udf = mlflow.pyfunc.spark_udf(\n  spark, \n  'runs:/{0}/model'.format(run.info.run_id), \n  result_type=result_schema\n  )\n\n# 上記の関数をSQLで使用するためにUDFとして登録する\n_ = spark.udf.register('clv', clv_udf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4305778-d0f3-452c-b585-5b6d30f77d1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["これでモデルがPython/SQLで利用可能になりました。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d114cb7-fddb-48e9-adb3-4ae31aa24d3f"}}},{"cell_type":"code","source":["# 次のセルでSQLを実行するためのtemp viewを作成しておく\nfiltered.createOrReplaceTempView('customer_metrics')\n\n# Spark DataFrameに関数を適用させる\ndisplay(\n  filtered\n    .withColumn(\n      'clv', \n      clv_udf(filtered.frequency, filtered.recency, filtered.T, filtered.monetary_value, lit(12), lit(0.01))\n      )\n    .selectExpr(\n      'customerid', \n      'clv'\n      )\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"031dc572-bfa3-44c1-8f40-d30fcadaade3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It can also be used with SQL:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d74b7e73-215b-4116-b114-ef69ec2f0997"}}},{"cell_type":"code","source":["%sql -- 顧客生涯価値を算出する\n\nSELECT\n  customerid,\n  clv(\n    frequency,\n    recency,\n    T,\n    monetary_value,\n    12,\n    0.01\n    ) as clv\nFROM customer_metrics;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5b08b94-46c5-486a-9b4a-7d09182ae15e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CLV Part 2: 将来消費を推定する","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3899882644116531}},"nbformat":4,"nbformat_minor":0}
