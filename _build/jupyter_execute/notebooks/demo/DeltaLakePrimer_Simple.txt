import re

# Username を取得。
username_raw = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')
# Username の英数字以外を除去し、全て小文字化。Usernameをファイルパスやデータベース名の一部で使用可能にするため。
username = re.sub('[^A-Za-z0-9]+', '_', username_raw).lower()
dbname = f'delta_db_{username}'

print(f'>>> username => {username}')
print(f'>>> dbname => {dbname}')

%fs head /databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv

df = (
  spark.read.format('csv')
  .option('Header', True)
  .option('inferSchema', True)
  .load('/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv') 
)

display(df)

df_cleaned = (
  df
  .where('price between 2000 and 2500') # カラムの条件
  .withColumn('magic_number', df.x * df.y + df.z ) # カラムの追加
  .select('carat', 'cut', 'color', 'clarity', 'price', 'magic_number') # カラムの抽出
)

display( df_cleaned )

( 
  df_cleaned.write
  .format('json')
  .mode('overwrite')
  .save(f'/tmp/{username}/diamonds_json')
)

display( dbutils.fs.ls(f'/tmp/{username}/diamonds_json') )

%fs head dbfs:/tmp/masahiko_kitamura_databricks_com/diamonds_json/part-00000-tid-8257997043505265100-c0ac53bf-7584-42ad-8753-836c9374b41f-2851-1-c000.json

df.createOrReplaceTempView('diamonds')

df_sql = sql('''
  SELECT 
    carat, cut, color, clarity, price, x * y + z as magic_number
  FROM diamonds
''')

display( df_sql )

# パス指定
source_path = 'dbfs:/databricks-datasets/samples/lending_club/parquet/'
delta_path = f'dbfs:/home/{username}/delta/lending-club-loan/'

# 既存のデータを削除
dbutils.fs.rm(delta_path, recurse=True)

# データを読み込む
df = spark.read.parquet(source_path)

# レコード数
print(df.count())

# randomSplit()を使って、5%のサンプルを読み取る
(data, data_rest) = df.randomSplit([0.05, 0.95], seed=123)

# 読み込まれたデータを参照
display( data )

from pandas_profiling import ProfileReport
df_profile = ProfileReport(df.toPandas(), minimal=True, title="Profiling Report", progress_bar=False, infer_dtypes=False)
profile_html = df_profile.to_html()

displayHTML(profile_html)

# 簡単な処理を行い、結果をParquetとして保存
from pyspark.sql.functions import col, expr

data.select('loan_amnt', 
            'term',
            'int_rate',
            'grade',
            'addr_state',
            'emp_title',
            'home_ownership',
            'annual_inc',
            'loan_status')\
  .withColumn('int_rate', expr('cast(replace(int_rate,"%","") as float)'))\
  .withColumnRenamed('addr_state', 'state')\
  .write\
  .format('delta')\
  .mode('overwrite')\
  .save(delta_path)

# Databaseを作成
sql(f'CREATE DATABASE IF NOT EXISTS {dbname}')
sql(f'USE {dbname}')

# テーブルとして登録
sql(f'DROP TABLE IF EXISTS LBS')
sql(f'CREATE TABLE LBS USING delta LOCATION "{delta_path}"')

%sql

-- データをSELECT文でクエリしてみよう
Select state, loan_status, count(*) as counts 
From LBS  
Group by state, loan_status
Order by counts desc

# %fs ls /home/{username}/delta/lending-club-loan/

display( 
  dbutils.fs.ls(f'{delta_path}') # <= Deltaを書き込んだディレクトリをリストしてみる
)

# %fs head /home/parquet/lending-club-loan/_delta_log/00000000000000000000.json

dbutils.fs.head(f'{delta_path}/_delta_log/00000000000000000000.json')

%sql
-- Describe History機能でデータの変更履歴を監査
Describe History LBS

%sql
-- マップで州ごとの融資総額を見てみよう
Select *
From LBS

%sql
-- 'WA'州を削除する
Delete From LBS Where State = 'WA';

-- データを見る
Select * 
From LBS

%sql
-- 'NY'州のローン申込額を20倍にする!
Update LBS Set loan_amnt = loan_amnt * 20 Where State = 'NY';

-- データを見る
Select *
From LBS 

# Demo用のマージするデータを別途作成 (本番環境では新規データがこれに値する)
columns = sql('Describe Table LBS').filter('data_type != ""').select('col_name').rdd.flatMap(lambda x: x).collect()

merge_df = sc.parallelize([
   [999999, '36 months', 5.00, 'A', 'IA', 'Demo Data', 'RENT', 1000000, 'Current']
]).toDF(columns)

merge_df.createOrReplaceTempView("merge_table")
display(merge_df)

%sql
-- マージオペレーションを行う
Merge Into LBS as target
Using merge_table as source
on target.State = source.State
when MATCHED Then Update Set *
When Not MATCHED Then Insert *
;

-- データを見る
Select *
From LBS 

# 既存のデータの10行を読み込んで、新たなカラムを追加
new_df = spark.read\
              .format('delta')\
              .load(delta_path)\
              .limit(10)\
              .withColumn('interest_flag', expr('case when int_rate >=6 then "high" else "low" end'))

display(new_df)

# スキーマが違うデータセットへ書き込む (Append)
new_df.write.format('delta').mode('append').save(delta_path)

# スキーマが違うデータセットへ書き込む (with スキーマエボリューション)
new_df.write.format('delta').option('mergeSchema','true').mode('append').save(delta_path)

%sql
describe LBS

%sql
-- データを見る
Select *
From LBS 

%sql
-- Describe History機能でデータの変更履歴を監査
Describe History LBS

%sql
-- バージョンを指定してスナップショットを取得
Select * 
From LBS Version AS OF 2

# 時間をしてしてスナップショットを取得
desiredTimestamp = spark.sql("Select timestamp From (Describe History LBS) Order By timestamp Desc").take(10)[-1].timestamp

print(f'desiredTimestamp => {desiredTimestamp}')

display(
  sql(f"Select * From LBS TIMESTAMP AS OF '{desiredTimestamp}'")
)

%sql
-- OPTIMIZEでデータをコンパクト化、インデックス作成。更にZORDERで良く使われるカラムのデータを物理的に一カ所へまとめる
OPTIMIZE LBS ZORDER By (state)

%sql
-- セル10と全く同じクエリを実行
Select state, loan_status, count(*) as counts 
From LBS
Group by state, loan_status
Order by counts desc

from pyspark.sql.types import *
from pyspark.sql.functions import from_json, col
import re

# ファイルパスの設定(書き込み先など)
## Username を取得。
username_raw = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')
## Username の英数字以外を除去し、全て小文字化。Usernameをファイルパスやデータベース名の一部で使用可能にするため。
username = re.sub('[^A-Za-z0-9]+', '_', username_raw).lower()

homeDir = f'/home/{username}/streaming/wikipedia/'
bronzePath = homeDir + "bronze.delta"
bronzeCkpt = homeDir + "bronze.checkpoint"

# 保存先をリセット
dbutils.fs.rm(homeDir, True)

#　JSONデータのスキーマ定義
schema = StructType([
  StructField("channel", StringType(), True),
  StructField("comment", StringType(), True),
  StructField("delta", IntegerType(), True),
  StructField("flag", StringType(), True),
  StructField("geocoding", StructType([
    StructField("city", StringType(), True),
    StructField("country", StringType(), True),
    StructField("countryCode2", StringType(), True),
    StructField("countryCode3", StringType(), True),
    StructField("stateProvince", StringType(), True),
    StructField("latitude", DoubleType(), True),
    StructField("longitude", DoubleType(), True),
  ]), True),
  StructField("isAnonymous", BooleanType(), True),
  StructField("isNewPage", BooleanType(), True),
  StructField("isRobot", BooleanType(), True),
  StructField("isUnpatrolled", BooleanType(), True),
  StructField("namespace", StringType(), True),         
  StructField("page", StringType(), True),              
  StructField("pageURL", StringType(), True),           
  StructField("timestamp", StringType(), True),        
  StructField("url", StringType(), True),
  StructField("user", StringType(), True),              
  StructField("userURL", StringType(), True),
  StructField("wikipediaURL", StringType(), True),
  StructField("wikipedia", StringType(), True),
])

#　JSONストリームを解析し、Deltaに保存
input_DF = (
  spark
  .readStream
  .format('kafka')                          # Kafkaをソースと指定
  .option('kafka.bootstrap.servers', 
          'server2.databricks.training:9092')
  .option('subscribe', 'en')
  .load()
)

# ELTをして、Deltaに書き込む
(
  input_DF
  .withColumn('json', from_json(col('value').cast('string'), schema))   # Kafkaのバイナリデータを文字列に変換し、from_json()でJSONをパース
  .select(col("json.*"))                    # JSONの子要素だけを取り出す
  .writeStream                              # writeStream()でストリームを書き出す
  .format('delta')                          # Deltaとして保存
  .option('checkpointLocation', bronzeCkpt) # チェックポイント保存先を指定
  .outputMode('append')                     # マイクロバッチの結果をAppendで追加
  .queryName('Bronze Stream')               # ストリームに名前を付ける（推奨）
  .start(bronzePath)                        # start()でストリーム処理を開始 (アクション)
)

# データフレームの確認
df = spark.read.format('delta').load(bronzePath)

display( df )

# データフレームの確認2
df = spark.readStream.format('delta').load(bronzePath)

display( df )

spark.readStream.format('delta').load(bronzePath).createOrReplaceTempView('tmp_wikipedia_msg')

%sql
SELECT geocoding.countryCode3, count(*) FROM tmp_wikipedia_msg
WHERE geocoding.countryCode3 is not null
GROUP BY geocoding.countryCode3
ORDER BY geocoding.countryCode3

sql(f'DROP DATABASE {dbname} CASCADE')
dbutils.fs.rm(delta_path, True)
dbutils.fs.rm(homeDir, True)
dbutils.fs.rm(f'/tmp/{username}', True)



